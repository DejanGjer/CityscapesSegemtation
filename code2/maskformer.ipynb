{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/maskformer-swin-base-ade\"\n",
    "batch_size = 2\n",
    "to_sample = True\n",
    "sample_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dejang/anaconda3/envs/transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\n",
    "from torchvision.transforms import ColorJitter\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from labels import labels\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, checkpoint, batch_size, to_sample=False, sample_size=80):\n",
    "        self.chekpoint = checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.to_sample = to_sample\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.train_ds, self.validation_ds, self.test_ds = self.load_or_download_dataset()\n",
    "        if to_sample:\n",
    "            self.train_ds = self.train_ds.select(range(sample_size))\n",
    "            self.validation_ds = self.validation_ds.select(range(sample_size))\n",
    "            self.test_ds = self.test_ds.select(range(sample_size))\n",
    "            print(\"Dataset is sampled\")\n",
    "        # create lebel2id and id2label dictionaries\n",
    "        self.label2id = {label.name: label.id for label in labels}\n",
    "        self.id2label = {label.id: label.name for label in labels}\n",
    "        # load image processor\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "        # self.image_processor = MaskFormerFeatureExtractor.from_pretrained(checkpoint)\n",
    "        self.jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n",
    "        # set format of datasets to torch\n",
    "        self.train_ds.set_format(\"torch\")\n",
    "        self.validation_ds.set_format(\"torch\")\n",
    "        self.test_ds.set_format(\"torch\")\n",
    "        print(self.train_ds.format)\n",
    "        # transform the dataset\n",
    "        self.train_ds_transformed = self.train_ds.with_transform(self.train_transforms)\n",
    "        self.validation_ds_transformed = self.validation_ds.with_transform(self.test_transforms)\n",
    "        self.test_ds_transformed = self.test_ds.with_transform(self.test_transforms) \n",
    "         \n",
    "        self.train_dataloader = DataLoader(self.train_ds_transformed, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "        self.validation_dataloader = DataLoader(self.validation_ds_transformed, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "        self.test_dataloader = DataLoader(self.test_ds_transformed, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "        self.original_trained_dataloader = DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "        self.original_validation_dataloader = DataLoader(self.validation_ds, batch_size=self.batch_size, shuffle=False)\n",
    "        self.original_test_dataloader = DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        example = next(iter(self.train_dataloader))\n",
    "        ex_pixel_values = example[\"pixel_values\"]\n",
    "        ex_pixel_mask = example[\"pixel_mask\"]\n",
    "        ex_mask_labels = example[\"mask_labels\"]\n",
    "        ex_class_labels = example[\"class_labels\"]\n",
    "        print(\"ONE EXAMPLE\")\n",
    "        print(type(ex_pixel_values))\n",
    "        print(ex_pixel_values.shape)\n",
    "        print(type(ex_pixel_mask))\n",
    "        print(ex_pixel_mask.shape)\n",
    "        for ex in ex_mask_labels:\n",
    "            print(ex.shape)\n",
    "        print(type(ex_class_labels))\n",
    "        for ex in ex_class_labels:\n",
    "            print(ex.shape)\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "        pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "        class_labels = [example[\"class_labels\"] for example in batch]\n",
    "        mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "        return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels}\n",
    "\n",
    "\n",
    "    def extract_single_channel(self, image):\n",
    "        # Split the image into channels (R, G, B)\n",
    "        r, _, _ = image.split()\n",
    "\n",
    "        # Create a new single-channel image using the channel you want (e.g., red channel)\n",
    "        return r\n",
    "\n",
    "    def train_transforms(self, example_batch):\n",
    "        images = [self.jitter(x) for x in example_batch[\"image\"]]\n",
    "        labels = [self.extract_single_channel(x) for x in example_batch[\"semantic_segmentation\"]]\n",
    "        print(\"Train transforms\")\n",
    "        print(f\"Batch size {len(images)}\")\n",
    "        print(type(images[0]))\n",
    "        print(type(labels[0]))\n",
    "        # print all unique labels for each image in the batch\n",
    "        for i in range(len(images)):\n",
    "            print(f\"Image {i}\")\n",
    "            print(f\"Unique labels: {np.unique(labels[i])}\")\n",
    "            print(len(np.unique(labels[i])))\n",
    "\n",
    "        # print(f\"Images shape {images[0].shape}\")\n",
    "        # print(f\"Labels shape {labels[0].shape}\")\n",
    "        inputs = self.image_processor(images, labels, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        pixel_mask = inputs[\"pixel_mask\"]\n",
    "        mask_labels = inputs[\"mask_labels\"]\n",
    "        class_labels = inputs[\"class_labels\"]\n",
    "        print(type(inputs))\n",
    "        print(inputs.keys())\n",
    "        print(type(pixel_values))\n",
    "        print(f\"Pixel values {pixel_values.shape}\")\n",
    "        print(type(pixel_mask))\n",
    "        print(f\"Pixel mask: {pixel_mask.shape}\")\n",
    "        print(type(mask_labels))\n",
    "        print(type(class_labels))\n",
    "        print(\"Mask labels\")\n",
    "        for ex in mask_labels:\n",
    "            print(ex.shape)\n",
    "        print(\"Class labels\")\n",
    "        for ex in class_labels:\n",
    "            print(ex.shape)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def test_transforms(self, example_batch):\n",
    "        images = [x for x in example_batch[\"image\"]]\n",
    "        labels = [self.extract_single_channel(x) for x in example_batch[\"semantic_segmentation\"]]\n",
    "        inputs = self.image_processor(images, labels)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def load_or_download_dataset(self):\n",
    "        dataset = load_dataset(\"Chris1/cityscapes\")\n",
    "        train_ds = dataset[\"train\"]\n",
    "        validation_ds = dataset[\"validation\"]\n",
    "        test_ds = dataset[\"test\"]\n",
    "        print(train_ds)\n",
    "        print(validation_ds)\n",
    "        print(test_ds)\n",
    "        return train_ds, validation_ds, test_ds\n",
    "    \n",
    "    def get_train_dataloader(self):\n",
    "        return self.train_dataloader\n",
    "    \n",
    "    def get_validation_dataloader(self):\n",
    "        return self.validation_dataloader\n",
    "    \n",
    "    def get_test_dataloader(self):\n",
    "        return self.test_dataloader\n",
    "    \n",
    "    def get_original_train_dataloader(self):\n",
    "        return self.original_trained_dataloader\n",
    "    \n",
    "    def get_original_validation_dataloder(self):\n",
    "        return self.original_validation_dataloader\n",
    "    \n",
    "    def get_original_test_dataloader(self):\n",
    "        return self.original_test_dataloader\n",
    "    \n",
    "    def get_num_labels(self):\n",
    "        return len(self.label2id) - 1 # we subtract one because we ignore -1 label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/dejang/.cache/huggingface/datasets/Chris1___parquet/Chris1--cityscapes-2bd50e1e8cc703b7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|██████████| 3/3 [00:00<00:00, 369.80it/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'semantic_segmentation'],\n",
      "    num_rows: 2975\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'semantic_segmentation'],\n",
      "    num_rows: 500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'semantic_segmentation'],\n",
      "    num_rows: 1525\n",
      "})\n",
      "Dataset is sampled\n",
      "{'type': 'torch', 'format_kwargs': {}, 'columns': ['image', 'semantic_segmentation'], 'output_all_columns': False}\n",
      "Train transforms\n",
      "Batch size 2\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "Image 0\n",
      "Unique labels: [ 1  3  4  7  8 11 17 19 20 21 22 23 24 25 26 28 33]\n",
      "17\n",
      "Image 1\n",
      "Unique labels: [ 0  1  3  4  5  7  8 11 17 20 21 23 24 25 26 33]\n",
      "16\n",
      "<class 'transformers.image_processing_utils.BatchFeature'>\n",
      "dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels'])\n",
      "<class 'torch.Tensor'>\n",
      "Pixel values torch.Size([2, 3, 640, 1280])\n",
      "<class 'torch.Tensor'>\n",
      "Pixel mask: torch.Size([2, 640, 1280])\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Mask labels\n",
      "torch.Size([17, 640, 1280])\n",
      "torch.Size([16, 640, 1280])\n",
      "Class labels\n",
      "torch.Size([17])\n",
      "torch.Size([16])\n",
      "ONE EXAMPLE\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 3, 640, 1280])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 640, 1280])\n",
      "torch.Size([17, 640, 1280])\n",
      "torch.Size([16, 640, 1280])\n",
      "<class 'list'>\n",
      "torch.Size([17])\n",
      "torch.Size([16])\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = Dataset(checkpoint, batch_size, to_sample, sample_size)\n",
    "train_ds = dataset.get_train_dataloader()\n",
    "validation_ds = dataset.get_validation_dataloader()\n",
    "test_ds = dataset.get_test_dataloader()\n",
    "num_labels = dataset.get_num_labels()\n",
    "\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/home/dejang/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": false,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 512,\n",
       "    \"width\": 512\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MaskFormerForInstanceSegmentation were not initialized from the model checkpoint at facebook/maskformer-swin-base-ade and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([36, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([36]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([36]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train transforms\n",
      "Batch size 2\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "Image 0\n",
      "Unique labels: [ 1  3  4  6  7  8 11 12 13 15 17 19 20 21 22 23 24 25 26 28 32 33]\n",
      "22\n",
      "Image 1\n",
      "Unique labels: [ 1  3  4  6  7  8  9 11 12 17 19 20 21 23 24 26 32]\n",
      "17\n",
      "<class 'transformers.image_processing_utils.BatchFeature'>\n",
      "dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels'])\n",
      "<class 'torch.Tensor'>\n",
      "Pixel values torch.Size([2, 3, 640, 1280])\n",
      "<class 'torch.Tensor'>\n",
      "Pixel mask: torch.Size([2, 640, 1280])\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Mask labels\n",
      "torch.Size([22, 640, 1280])\n",
      "torch.Size([17, 640, 1280])\n",
      "Class labels\n",
      "torch.Size([22])\n",
      "torch.Size([17])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = MaskFormerForInstanceSegmentation.from_pretrained(checkpoint, id2label=dataset.id2label, label2id=dataset.label2id, ignore_mismatched_sizes=True)\n",
    "# get one example from train set\n",
    "example = next(iter(train_ds))\n",
    "pixel_values = example[\"pixel_values\"]\n",
    "pixel_mask = example[\"pixel_mask\"]\n",
    "mask_labels = example[\"mask_labels\"]\n",
    "class_labels = example[\"class_labels\"]\n",
    "# get output from model\n",
    "outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask, mask_labels=mask_labels, class_labels=class_labels)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
